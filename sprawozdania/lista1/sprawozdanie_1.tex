\documentclass{article}
\usepackage[final]{nips_2017}
\usepackage{polski}
\usepackage[utf8]{inputenc}    % allow utf-8 input
\usepackage[T1]{fontenc}       % use 8-bit T1 fonts
\usepackage{hyperref}          % hyperlinks
\usepackage{url}               % simple URL typesetting
\usepackage{booktabs}          % professional-quality tables
\usepackage{amsfonts}          % blackboard math symbols
\usepackage{nicefrac}          % compact symbols for 1/2, etc.
\usepackage{microtype}         % microtypography
\usepackage[section]{placeins} % figures kept in sections
\usepackage{graphicx}          % images
\graphicspath{ {./img/} }
\usepackage{multirow}

\renewcommand{\figurename}{Wykres}

\title{  Perceptron prosty i Adaline\\Sieci Neuronowe 2020 }

\author{
  Jakub Ciszek \\
  238035\\
}

\begin{document}

\maketitle

\newpage
\tableofcontents
\newpage

Cały kod wykorzystany w zadaniu znajduje się pod adresem: \url{https://github.com/Greenpp/sieci-neuronowe-pwr-2020}

\section{Opis badań}
\subsection{Plan eksperymentów}

Wszystkie eksperymenty zostały przeprowadzone 100 razy. Losowość przy inicjalizacji wag oraz generacji danych nie została narzucona żadnym ziarnem. Podczas badań przyjęto górną granicę 1000 epok, po przekroczeniu której, uczenie zostawało przerywane. Jeśli  model przekroczył tą granice przy pierwszych 6 wyuczeniach, zostawał klasyfikowany jako niezdolny do wyuczenia i pomijany. Zgodnie z instrukcją zostały przeprowadzone następujące badania:
\begin{itemize}
	\item Wpływ wartości progu theta na szybkość uczenia Perceptronu
	\item Wpływ zakresu inicjalizacji wag na szybkość uczenia Perceptronu
	\item Wpływ wartości współczynnika uczenia alpha na szybkość uczenia Perceptronu
	\item Wpływ funkcji aktywacyjnej (unipolarna, bipolarna) na szybkość uczenia Perceptronu
	\item Wpływ zakresu inicjalizacji wag na szybkość uczenia Adaline
	\item Wpływ wartości współczynnika uczenia alpha na szybkość uczenia Adaline
	\item Wpływ przyjętego dopuszczalnego błędu na wynik uczenia w Adaline
	\item Porównanie Perceptronu i Adaline
\end{itemize}

\subsection{Charakterystyka zbiorów danych}

Dane użyte w zadaniu są reprezentacją logicznej funkcji AND

\begin{table}[!h]
	\caption{Wartości funkcji AND}
	\label{tabela-and}
	\centering
	\begin{tabular}{llc}
		\toprule
		\(a\) & \(b\) & \(a \land b\) \\
		\midrule
		0     & 0     & 0             \\
		0     & 1     & 0             \\
		1     & 1     & 1             \\
		1     & 0     & 0             \\
		\bottomrule
	\end{tabular}
\end{table}

W trakcie eksperymentów wykorzystano następujące zbiory:
\begin{itemize}
	\item Zbiór uczący: 4 podstawowe wzorce oraz 4-krotna kopia każdego z nich z przesuniętymi wartościami wejściowymi o \(\pm\ $-0.01 -- 0.01$\)
	\item Zbiór walidujący: 4 podstawowe wzorce
	\item Zbiór testowy: 4 podstawowe wzorce
\end{itemize}

\newpage
\section{Eksperymenty}

\subsection{Wpływ wartości progu theta na szybkość uczenia Perceptronu}
\subsubsection*{Założenia}
\begin{table}[!h]
	\caption{Stałe dla eksperymentu 1}
	\label{tabela-const-1}
	\centering
	\begin{tabular}{lr}
		\toprule
		Parametr               & Wartość         \\
		\midrule
		Bias                   & Nie               \\
		Zakres wag             & \($-0.2 -- 0.2$\) \\
		Współczynnik uczenia & 0.01              \\
		\bottomrule
	\end{tabular}
\end{table}

Zmienną w tym eksperymencie była wartość theta dla funkcji aktywacji. Przyjmowała wartości ze zbioru \(\{$-1.0, -0.8, -0.5, -0.2, 0.0, 0.2, 0.5, 0.8, 1.0$\}\)
\subsubsection*{Przebieg}

Podczas eksperymentu model został zainicjalizowany 100 razy dla każdej z badanych wartości oraz wyuczony, uzyskane wyniki zostały zapisane w postaci pliku .plk do dalszej analizy. Badanie przeprowadzono dla funkcji aktywacyjnej unipolarnej jak i bipolarnej.

\subsubsection*{Wyniki}
\begin{figure}[!h]
	\centering
	\caption{Zależność szybkości uczenia od parametru theta}
	\includegraphics[width=\textwidth]{per_theta.png}
	\label{fig:res11}
\end{figure}

\begin{table}[!h]
	\caption{Średnia ilość epok potrzebna do wyuczenia w zależności od parametru theta}
	\label{tabela-res-11}
	\centering
	\begin{tabular}{rrr}
		\toprule
		\multirow{2}{*}{Theta}   & \multicolumn{2}{c}{Epoki} \\
		     & bipolarna     & unipolarna    \\
		\midrule
		-1.0 & -             & -             \\
		-0.8 & -             & -             \\
		-0.5 & -             & -             \\
		-0.2 & -             & -             \\
		0.0  & -             & -             \\
		0.2  & \textbf{2.27} & \textbf{5.47} \\
		0.5  & 3.88          & 6.77          \\
		0.8  & 5.64          & 9.46          \\
		1.0  & 6.36          & 11.57         \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection*{Wnioski}

Z otrzymanych wyników, widocznych na wykresie~\ref{fig:res11} oraz tabeli~\ref{tabela-res-11}, wynika ze wyuczenie modelu nie było możliwe dla progu \(theta \leq  0 \). Jest to zgodne z jego matematyczna interpretacją jako funkcji liniowej. Przy progu równym zero nie jest możliwe oderwanie od początka układu współrzędnych co uniemożliwia rozdzielenie zadanych punktów. Podczas gdy ujemne wartości sprawiają, że dla wymaganej orientacji prostej suma ujemnych wag jest większa od wartości progu co powoduje odwrócenie otrzymywanych klas. Kolejną obserwacją jest mniejsza ilość epok potrzebna do wyuczenia przy mniejszym, dodatnim progu. Ta zależność także wynika z charakterystyki równania liniowego, które do zmiany orientacji potrzebuje znacznie mniejszych współczynników a i b dla małego c, co powoduje zmniejszenie liczby kroków, a z tym epok.

\newpage
\subsection{Wpływ zakresu inicjalizacji wag na szybkość uczenia Perceptronu}
\subsubsection*{Założenia}
\begin{table}[!h]
	\caption{Stałe dla eksperymentu 2}
	\label{tabela-const-2}
	\centering
	\begin{tabular}{lr}
		\toprule
		Parametr               & Wartość \\
		\midrule
		Bias                   & Tak       \\
		Theta                  & 0.0       \\
		Współczynnik uczenia & 0.01      \\
		\bottomrule
	\end{tabular}
\end{table}

Zmienną w tym eksperymencie była wartość początkowego zakresu wag. Przyjmowała wartości ze zbioru \(\{$0.0, -0.1 -- 0.1, -0.2 -- 0.2, -0.5 -- 0.5, -0.8 -- 0.8, -1.0 -- 1.0$\}\)
\subsubsection*{Przebieg}

Podczas eksperymentu model został zainicjalizowany 100 razy dla każdej z badanych wartości oraz wyuczony, uzyskane wyniki zostały zapisane w postaci pliku .plk do dalszej analizy. Badanie przeprowadzono dla funkcji aktywacyjnej unipolarnej jak i bipolarnej.

\subsubsection*{Wyniki}
\begin{figure}[!h]
	\centering
	\caption{Zależność szybkości uczenia od początkowego zakresu wag}
	\includegraphics[width=\textwidth]{per_w.png}
	\label{fig:res2}
\end{figure}

\begin{table}[!h]
	\caption{Średnia ilość epok potrzebna do wyuczenia w zależności od początkowego zakresu wag}
	\label{tabela-res-2}
	\centering
	\begin{tabular}{rrr}
		\toprule
		\multirow{2}{*}{Zakres wag}   & \multicolumn{2}{c}{Epoki} \\
		                  & bipolarna     & unipolarna    \\
		\midrule
		0.0               & \textbf{1.79} & \textbf{4.62} \\
		\($-0.1 -- 0.1$\) & 2.83          & 8.44          \\
		\($-0.2 -- 0.2$\) & 3.62          & 13.00         \\
		\($-0.5 -- 0.5$\) & 7.14          & 24.01         \\
		\($-0.8 -- 0.8$\) & 9.29          & 41.41         \\
		\($-1.0 -- 1.0$\) & 13.00         & 52.40         \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection*{Wnioski}

Z otrzymanych wyników, widocznych na wykresie~\ref{fig:res2} oraz tabeli~\ref{tabela-res-2}, wynika że uczenie następowało dużo szybciej dla początkowych wartości wag bliższych 0. Te wyniki zgadzają się z zachowaniem modelu jako równania liniowego - przy mniejszych wartościach potrzebne są mniejsze zmiany aby odpowiednio zorientować prostą. Takie wyniki jednak nie powinny wystąpić podczas badania głębszych modeli gdzie początkowe wartości równe 0 mogą prowadzić do jednakowych zmian wszystkich wag w modelu i upośledzenia jego możliwości uczenia.

\newpage
\subsection{Wpływ wartości współczynnika uczenia alpha na szybkość uczenia Perceptronu}
\subsubsection*{Założenia}

\begin{table}[!h]
	\caption{Stałe dla eksperymentu 3}
	\label{tabela-const-3}
	\centering
	\begin{tabular}{lr}
		\toprule
		Parametr   & Wartość         \\
		\midrule
		Bias       & Tak               \\
		Theta      & 0.0               \\
		Zakres wag & \($-0.5 -- 0.5$\) \\
		\bottomrule
	\end{tabular}
\end{table}

Zmienną w tym eksperymencie była wartość współczynnika uczenia. Przyjmowała wartości ze zbioru \(\{$0.0001, 0.001, 0.01, 0.1, 1.0$\}\)

\subsubsection*{Przebieg}

Podczas eksperymentu model został zainicjalizowany 100 razy dla każdej z badanych wartości oraz wyuczony, uzyskane wyniki zostały zapisane w postaci pliku .plk do dalszej analizy. Badanie przeprowadzono dla funkcji aktywacyjnej unipolarnej jak i bipolarnej.

\subsubsection*{Wyniki}

\begin{figure}[!h]
	\centering
	\caption{Zależność szybkości uczenia od parametru alpha}
	\includegraphics[width=\textwidth]{per_alpha.png}
	\label{fig:res3}
\end{figure}

\begin{table}[!h]
	\caption{Średnia ilość epok potrzebna do wyuczenia w zależności od parametru alpha}
	\label{tabela-res-3}
	\centering
	\begin{tabular}{rrr}
		\toprule
		\multirow{2}{*}{Alpha}   & \multicolumn{2}{c}{Epoki} \\
		       & bipolarna     & unipolarna    \\
		\midrule
		0.0001 & 495.44        & 836.04        \\
		0.0010 & 57.52         & 223.90        \\
		0.0100 & 6.88          & 22.60         \\
		0.1000 & 2.33          & 6.80          \\
		1.0000 & \textbf{1.86} & \textbf{5.68} \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection*{Wnioski}

Z otrzymanych wyników, widocznych na wykresie~\ref{fig:res3} oraz tabeli~\ref{tabela-res-3}, wynika że mniejsze wartości współczynnika uczenia spowalniają ten proces. Z jednej strony takie wyniki są poprawne, ponieważ mniejszy współczynnik przyczynia się do mniejszy zmian wag, jednak z drugiej wysoka wartość nie pozwala na tak dobre wyuczenie modelu. W tym przypadku dokładność nie była potrzebna, co powoduje że najlepsze efekty osiągnął wysoki współczynnik alpha.

\newpage
\subsection{Wpływ funkcji aktywacyjnej (unipolarna, bipolarna) na szybkość uczenia Perceptronu}
\subsubsection*{Wyniki}

\begin{table}[!h]
	\caption{Średnia ilość epok potrzebna do wyuczenia w zależności od parametru theta}
	\label{tabela-res-41}
	\centering
	\begin{tabular}{rrr}
		\toprule
		\multirow{2}{*}{Theta}   & \multicolumn{2}{c}{Epoki} \\
		     & bipolarna     & unipolarna \\
		\midrule
		-1.0 & -             & -          \\
		-0.8 & -             & -          \\
		-0.5 & -             & -          \\
		-0.2 & -             & -          \\
		0.0  & -             & -          \\
		0.2  & \textbf{2.27} & 5.47       \\
		0.5  & \textbf{3.88} & 6.77       \\
		0.8  & \textbf{5.64} & 9.46       \\
		1.0  & \textbf{6.36} & 11.57      \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[!h]
	\caption{Średnia ilość epok potrzebna do wyuczenia w zależności od początkowego zakresu wag}
	\label{tabela-res-42}
	\centering
	\begin{tabular}{rrr}
		\toprule
		\multirow{2}{*}{Zakres wag}   & \multicolumn{2}{c}{Epoki} \\
		                  & bipolarna      & unipolarna \\
		\midrule
		0.0               & \textbf{1.79}  & 4.62       \\
		\($-0.1 -- 0.1$\) & \textbf{2.83}  & 8.44       \\
		\($-0.2 -- 0.2$\) & \textbf{3.62}  & 13.00      \\
		\($-0.5 -- 0.5$\) & \textbf{7.14}  & 24.01      \\
		\($-0.8 -- 0.8$\) & \textbf{9.29}  & 41.41      \\
		\($-1.0 -- 1.0$\) & \textbf{13.00} & 52.40      \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[!h]
	\caption{Średnia ilość epok potrzebna do wyuczenia w zależności od parametru alpha}
	\label{tabela-res-43}
	\centering
	\begin{tabular}{rrr}
		\toprule
		\multirow{2}{*}{Alpha}   & \multicolumn{2}{c}{Epoki} \\
		       & bipolarna       & unipolarna \\
		\midrule
		0.0001 & \textbf{495.44} & 836.04     \\
		0.0010 & \textbf{57.52}  & 223.90     \\
		0.0100 & \textbf{6.88}   & 22.60      \\
		0.1000 & \textbf{2.33}   & 6.80       \\
		1.0000 & \textbf{1.86}   & 5.68       \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection*{Wnioski}

Z wyników otrzymanych w poprzednich badaniach, widocznych w tabelach~\ref{tabela-res-41},~\ref{tabela-res-42} i~\ref{tabela-res-43}, wynika jednoznacznie, że lepsze efekty otrzymuje się przy użyciu funkcji bipolarnej. Powodem tego jest o wiele mniejsza odległość od początku układu współrzędnych do obszaru w którym powinna znaleźć się separująca dane linia, co zmniejsza liczbę potrzebnych na przesunięcie jej tam epok. 

\newpage
\subsection{Wpływ zakresu inicjalizacji wag na szybkość uczenia Adaline}
\subsubsection*{Założenia}
\begin{table}[!h]
	\caption{Stałe dla eksperymentu 5}
	\label{tabela-const-5}
	\centering
	\begin{tabular}{lr}
		\toprule
		Parametr               & Wartość \\
		\midrule
		Bias                   & Tak       \\
		Theta                  & 0.0       \\
		Współczynnik uczenia & 0.01      \\
		Epsilon                & 0.2       \\
		\bottomrule
	\end{tabular}
\end{table}

Zmienną w tym eksperymencie była wartość początkowego zakresu wag. Przyjmowała wartości ze zbioru \(\{$0.0, -0.1 -- 0.1, -0.2 -- 0.2, -0.5 -- 0.5, -0.8 -- 0.8, -1.0 -- 1.0$\}\)

\subsubsection*{Przebieg}

Podczas eksperymentu model został zainicjalizowany 100 razy dla każdej z badanych wartości oraz wyuczony, uzyskane wyniki zostały zapisane w postaci pliku .plk do dalszej analizy.
\subsubsection*{Wyniki}

\begin{figure}[!h]
	\centering
	\caption{Zależność szybkości uczenia od początkowego zakresu wag}
	\includegraphics[width=.5\textwidth]{ada_w.png}
	\label{fig:res5}
\end{figure}

\begin{table}[!h]
	\caption{Średnia ilość epok potrzebna do wyuczenia w zależności od początkowego zakresu wag}
	\label{tabela-res-5}
	\centering
	\begin{tabular}{rrr}
		\toprule
		Zakres wag        & Epoki         \\
		\midrule
		0.0               & \textbf{1.00} \\
		\($-0.1 -- 0.1$\) & 1.61          \\
		\($-0.2 -- 0.2$\) & 2.51          \\
		\($-0.5 -- 0.5$\) & 5.93          \\
		\($-0.8 -- 0.8$\) & 11.54         \\
		\($-1.0 -- 1.0$\) & 11.95         \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection*{Wnioski}

Z otrzymanych wyników, widocznych na wykresie~\ref{fig:res5} oraz tabeli~\ref{tabela-res-5}, wynika że uczenie następowało dużo szybciej dla początkowych wartości wag bliższych 0. Te wyniki zgadzają się z zachowaniem modelu jako równania liniowego - przy mniejszych wartościach potrzebne są mniejsze zmiany aby odpowiednio zorientować prostą. Takie wyniki jednak nie powinny wystąpić podczas badania głębszych modeli gdzie początkowe wartości równe 0 mogą prowadzić do jednakowych zmian wszystkich wag w modelu i upośledzenia jego możliwości uczenia.

\newpage
\subsection{Wpływ wartości współczynnika uczenia alpha na szybkość uczenia Adaline}
\subsubsection*{Założenia}

\begin{table}[!h]
	\caption{Stałe dla eksperymentu 6}
	\label{tabela-const-6}
	\centering
	\begin{tabular}{lr}
		\toprule
		Parametr   & Wartość         \\
		\midrule
		Bias       & Tak               \\
		Theta      & 0.0               \\
		Zakres wag & \($-0.5 -- 0.5$\) \\
		Epsilon    & 0.2               \\
		\bottomrule
	\end{tabular}
\end{table}

Zmienną w tym eksperymencie była wartość współczynnika uczenia. Przyjmowała wartości ze zbioru \(\{$0.0001, 0.001, 0.01, 0.1, 1.0$\}\)

\subsubsection*{Przebieg}

Podczas eksperymentu model został zainicjalizowany 100 razy dla każdej z badanych wartości oraz wyuczony, uzyskane wyniki zostały zapisane w postaci pliku .plk do dalszej analizy.

\subsubsection*{Wyniki}

\begin{figure}[!h]
	\centering
	\caption{Zależność szybkości uczenia od parametru alpha}
	\includegraphics[width=.5\textwidth]{ada_alpha.png}
	\label{fig:res6}
\end{figure}

\begin{table}[!h]
	\caption{Średnia ilość epok potrzebna do wyuczenia w zależności od parametru alpha}
	\label{tabela-res-6}
	\centering
	\begin{tabular}{rrr}
		\toprule
		Alpha  & Epoki         \\
		\midrule
		0.0001 & 567.96        \\
		0.0010 & 55.87         \\
		0.0100 & 5.56          \\
		0.1000 & \textbf{1.64} \\
		1.0000 & 20.81         \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection*{Wnioski}

Z otrzymanych wyników, widocznych na wykresie~\ref{fig:res6} oraz tabeli~\ref{tabela-res-6}, wynika że optymalnym współczynnikiem w przypadku Adaline jest 0.1. Jest to zapewne spowodowane możliwością otrzymania większych błędów niż w przypadku Perceptronu, gdzie ich zeskalowanie w dół pozwala na szybsze osiągnięcie, bardzo bliskiego środka układu, obszaru umożliwiającego podział danych.

\newpage
\subsection{Wpływ przyjętego dopuszczalnego błędu na wynik uczenia w Adaline}
\subsubsection*{Założenia}
\begin{table}[!h]
	\caption{Stałe dla eksperymentu 7}
	\label{tabela-const-7}
	\centering
	\begin{tabular}{lr}
		\toprule
		Parametr               & Wartość         \\
		\midrule
		Bias                   & Tak               \\
		Theta                  & 0.0               \\
		Zakres wag             & \($-0.5 -- 0.5$\) \\
		Współczynnik uczenia & 0.01              \\
		Epsilon                & 0.0               \\
		\bottomrule
	\end{tabular}
\end{table}
\subsubsection*{Przebieg}

Podczas eksperymentu model został zainicjalizowany 100 raz, po czym zostało sprawdzone zachowanie błędu. uzyskane wyniki zostały zapisane w postaci pliku .plk do dalszej analizy. Zapisane zostały błędy:
\begin{itemize}
	\item walidacyjny - obliczany w tym samym miejscu co błąd treningowy
	\item testowy - obliczany już po progowaniu
\end{itemize}

\subsubsection*{Wyniki}

\begin{figure}[!h]
	\centering
	\caption{Zachowanie błędu w trakcie uczenia Adaline}
	\includegraphics[width=.5\textwidth]{ada_epsilon.png}
	\label{fig:res7}
\end{figure}

Minimalna zaobserwowana wartość błędu walidacji: \($0.1303$\)

\subsubsection*{Wnioski}

Uzyskane wyniki, widoczne na wykresie~\ref{fig:res7} zostały ograniczone do 20 epok z powodu zerowego błędu testowego podczas dalszego uczenia. Minimalny otrzymany błąd po odwróceniu funkcji błędu daje wartość \(\simeq\ $1$\). Natomiast na wspomnianym wykresie widać, że błąd testowy przyjmuje zero w miarę zbliżania się błędu walidacyjnego do 0.2, co po takim samym przeliczeniu daje wartość błędu \(\simeq\ $1.27$\) na punkt. Wartości te można by zapewne otrzymać znajdując minimalne oraz optymalne ustawienie linii rozwiązując układ równań, ale z powodu braku czasu ten etap został pominięty.

\newpage
\subsection{Porównanie Perceptronu i Adaline}
\subsubsection*{Wyniki}

\begin{figure}[!h]
	\centering
	\caption{Porównanie szybkości uczenia Perceptronu i Adaline w zależności od początkowego zakresu wag}
	\includegraphics[width=\textwidth]{ada_per_w.png}
	\label{fig:res81}
\end{figure}

\begin{table}[!h]
	\caption{Porównanie średniej ilości epok potrzebnych do wyuczenia Perceptronu i Adaline w zależności od początkowego zakresu wag}
	\label{tabela-res-81}
	\centering
	\begin{tabular}{rrr}
		\toprule
		\multirow{2}{*}{Zakres wag}   & \multicolumn{2}{c}{Epoki} \\
		                  & Perceptron & Adaline        \\
		\midrule
		0.0               & 1.79       & \textbf{1.00}  \\
		\($-0.1 -- 0.1$\) & 2.83       & \textbf{1.61}  \\
		\($-0.2 -- 0.2$\) & 3.62       & \textbf{2.51}  \\
		\($-0.5 -- 0.5$\) & 7.14       & \textbf{5.93}  \\
		\($-0.8 -- 0.8$\) & 9.29       & \textbf{11.54} \\
		\($-1.0 -- 1.0$\) & 13.00      & \textbf{11.95} \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{figure}[!h]
	\centering
	\caption{Porównanie szybkości uczenia Perceptronu i Adaline w zależności od parametru alpha}
	\includegraphics[width=\textwidth]{ada_per_alpha.png}
	\label{fig:res82}
\end{figure}

\begin{table}[!h]
	\caption{Porównanie średniej ilości epok potrzebnych do wyuczenia Perceptronu i Adaline w zależności od parametru alpha}
	\label{tabela-res-82}
	\centering
	\begin{tabular}{rrr}
		\toprule
		\multirow{2}{*}{Alpha}   & \multicolumn{2}{c}{Epoki} \\
		       & Perceptron      & Adaline        \\
		\midrule
		0.0001 & \textbf{495.44} & 567.96         \\
		0.0010 & 57.52           & \textbf{55.87} \\
		0.0100 & 6.88            & \textbf{5.56}  \\
		0.1000 & 2.33            & \textbf{1.64}  \\
		1.0000 & \textbf{1.86}   & 20.81          \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection*{Wnioski}

Z wyników otrzymanych w poprzednich badaniach, widocznych na wykresach~\ref{tabela-res-81} i~\ref{tabela-res-82} oraz w tabelach~\ref{fig:res81} i~\ref{fig:res82}, wynika, że lepiej radzi sobie Adaline. Powodem tego jest najprawdopodobniej bardziej plastyczny sygnał zwrotny w tym modelu, który nie przyjmuje wartości dyskretnych jak w przypadku Perceptronu.

\newpage
\section{Wnioski}

\begin{itemize}
	\item Perceptron tak jak i Adaline umożliwiają klasyfikację w problemach rozdzielnych liniowo.
	\item Możliwość uczenia względem ciągłego błędu oraz użycie logiki bipolarnej pozwala Adaline na uzyskanie lepszych wyników w tym zadaniu.
	\item Odpowiednia konfiguracja hiperparametrów może znacząco przyspieszyć proces uczenia, lub w ogóle go umożliwić.
	\item Odpowiednia reprezentacja danych ma duży wpływ na proces uczenia.
\end{itemize}

\end{document}